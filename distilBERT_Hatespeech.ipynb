{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Framework and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install accelerate -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, matthews_corrcoef, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14b5dff1ccf64f8484452a216f710762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id  label                                              tweet\n",
      "0   6088      0   who cares th ones a thirty dirty argentina &a...\n",
      "1   5283      1   msnbc the same ms who said blocking flights f...\n",
      "2  18442      0  it  i hate a bitch dat act like a night. &128530;\n",
      "3   6715      0                          ha ha ha no good fat duke\n",
      "4  13003      0  lynch mob thus threaten zimmerman jury; &8216;...\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "hatespeech = pd.read_csv('/Users/lauraguinandrodriguez/Desktop/Hate Speech/twits_25k_balanced.csv')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(hatespeech.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_data, test_data = train_test_split(hatespeech, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "class HateSpeechDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Tokenize the data\n",
    "encodings = tokenizer(train_data['tweet'].tolist(), truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Create the Torch dataset\n",
    "dataset = HateSpeechDataset(encodings, train_data['label'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script distro.exe is installed in 'C:\\Users\\PabloFabianFaundezGa\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script httpx.exe is installed in 'C:\\Users\\PabloFabianFaundezGa\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script openai.exe is installed in 'C:\\Users\\PabloFabianFaundezGa\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collecting openai\n",
      "  Downloading openai-1.7.2-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.2.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Downloading pydantic-2.5.3-py3-none-any.whl.metadata (65 kB)\n",
      "     ---------------------------------------- 65.6/65.6 kB 1.8 MB/s eta 0:00:00\n",
      "Collecting sniffio (from openai)\n",
      "  Downloading sniffio-1.3.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from openai) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from openai) (4.8.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: certifi in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.14.6 (from pydantic<3,>=1.9.0->openai)\n",
      "  Downloading pydantic_core-2.14.6-cp311-none-win_amd64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Downloading openai-1.7.2-py3-none-any.whl (212 kB)\n",
      "   ---------------------------------------- 212.1/212.1 kB 6.5 MB/s eta 0:00:00\n",
      "Downloading anyio-4.2.0-py3-none-any.whl (85 kB)\n",
      "   ---------------------------------------- 85.5/85.5 kB 5.0 MB/s eta 0:00:00\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Downloading httpx-0.26.0-py3-none-any.whl (75 kB)\n",
      "   ---------------------------------------- 75.9/75.9 kB 4.4 MB/s eta 0:00:00\n",
      "Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
      "   ---------------------------------------- 76.9/76.9 kB 4.4 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.5.3-py3-none-any.whl (381 kB)\n",
      "   ---------------------------------------- 381.9/381.9 kB 7.9 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.14.6-cp311-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 1.9/1.9 MB 7.5 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: sniffio, pydantic-core, httpcore, distro, annotated-types, pydantic, anyio, httpx, openai\n",
      "Successfully installed annotated-types-0.6.0 anyio-4.2.0 distro-1.9.0 httpcore-1.0.2 httpx-0.26.0 openai-1.7.2 pydantic-2.5.3 pydantic-core-2.14.6 sniffio-1.3.0\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d101e690cdc34916af31363e4f08e390",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/645 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1105, 'learning_rate': 1.0000000000000002e-06, 'epoch': 0.05}\n",
      "{'loss': 1.0917, 'learning_rate': 2.0000000000000003e-06, 'epoch': 0.09}\n",
      "{'loss': 1.09, 'learning_rate': 3e-06, 'epoch': 0.14}\n",
      "{'loss': 1.1104, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.19}\n",
      "{'loss': 1.0954, 'learning_rate': 5e-06, 'epoch': 0.23}\n",
      "{'loss': 1.095, 'learning_rate': 6e-06, 'epoch': 0.28}\n",
      "{'loss': 1.0645, 'learning_rate': 7.000000000000001e-06, 'epoch': 0.33}\n",
      "{'loss': 1.0632, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.37}\n",
      "{'loss': 1.035, 'learning_rate': 9e-06, 'epoch': 0.42}\n",
      "{'loss': 1.0234, 'learning_rate': 1e-05, 'epoch': 0.47}\n",
      "{'loss': 0.9707, 'learning_rate': 1.1000000000000001e-05, 'epoch': 0.51}\n",
      "{'loss': 0.9202, 'learning_rate': 1.2e-05, 'epoch': 0.56}\n",
      "{'loss': 0.8844, 'learning_rate': 1.3000000000000001e-05, 'epoch': 0.6}\n",
      "{'loss': 0.7973, 'learning_rate': 1.4000000000000001e-05, 'epoch': 0.65}\n",
      "{'loss': 0.7884, 'learning_rate': 1.5e-05, 'epoch': 0.7}\n",
      "{'loss': 0.7406, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.74}\n",
      "{'loss': 0.6864, 'learning_rate': 1.7000000000000003e-05, 'epoch': 0.79}\n",
      "{'loss': 0.7318, 'learning_rate': 1.8e-05, 'epoch': 0.84}\n",
      "{'loss': 0.7208, 'learning_rate': 1.9e-05, 'epoch': 0.88}\n",
      "{'loss': 0.671, 'learning_rate': 2e-05, 'epoch': 0.93}\n",
      "{'loss': 0.6307, 'learning_rate': 2.1e-05, 'epoch': 0.98}\n",
      "{'loss': 0.5433, 'learning_rate': 2.2000000000000003e-05, 'epoch': 1.02}\n",
      "{'loss': 0.6159, 'learning_rate': 2.3000000000000003e-05, 'epoch': 1.07}\n",
      "{'loss': 0.5599, 'learning_rate': 2.4e-05, 'epoch': 1.12}\n",
      "{'loss': 0.4801, 'learning_rate': 2.5e-05, 'epoch': 1.16}\n",
      "{'loss': 0.5837, 'learning_rate': 2.6000000000000002e-05, 'epoch': 1.21}\n",
      "{'loss': 0.5062, 'learning_rate': 2.7000000000000002e-05, 'epoch': 1.26}\n",
      "{'loss': 0.5355, 'learning_rate': 2.8000000000000003e-05, 'epoch': 1.3}\n",
      "{'loss': 0.5155, 'learning_rate': 2.9e-05, 'epoch': 1.35}\n",
      "{'loss': 0.5346, 'learning_rate': 3e-05, 'epoch': 1.4}\n",
      "{'loss': 0.5418, 'learning_rate': 3.1e-05, 'epoch': 1.44}\n",
      "{'loss': 0.5144, 'learning_rate': 3.2000000000000005e-05, 'epoch': 1.49}\n",
      "{'loss': 0.6161, 'learning_rate': 3.3e-05, 'epoch': 1.53}\n",
      "{'loss': 0.5925, 'learning_rate': 3.4000000000000007e-05, 'epoch': 1.58}\n",
      "{'loss': 0.5714, 'learning_rate': 3.5e-05, 'epoch': 1.63}\n",
      "{'loss': 0.5109, 'learning_rate': 3.6e-05, 'epoch': 1.67}\n",
      "{'loss': 0.4573, 'learning_rate': 3.7e-05, 'epoch': 1.72}\n",
      "{'loss': 0.3474, 'learning_rate': 3.8e-05, 'epoch': 1.77}\n",
      "{'loss': 0.5315, 'learning_rate': 3.9000000000000006e-05, 'epoch': 1.81}\n",
      "{'loss': 0.5398, 'learning_rate': 4e-05, 'epoch': 1.86}\n",
      "{'loss': 0.646, 'learning_rate': 4.1e-05, 'epoch': 1.91}\n",
      "{'loss': 0.5351, 'learning_rate': 4.2e-05, 'epoch': 1.95}\n",
      "{'loss': 0.5525, 'learning_rate': 4.3e-05, 'epoch': 2.0}\n",
      "{'loss': 0.409, 'learning_rate': 4.4000000000000006e-05, 'epoch': 2.05}\n",
      "{'loss': 0.4366, 'learning_rate': 4.5e-05, 'epoch': 2.09}\n",
      "{'loss': 0.3467, 'learning_rate': 4.600000000000001e-05, 'epoch': 2.14}\n",
      "{'loss': 0.4679, 'learning_rate': 4.7e-05, 'epoch': 2.19}\n",
      "{'loss': 0.4552, 'learning_rate': 4.8e-05, 'epoch': 2.23}\n",
      "{'loss': 0.4321, 'learning_rate': 4.9e-05, 'epoch': 2.28}\n",
      "{'loss': 0.4211, 'learning_rate': 5e-05, 'epoch': 2.33}\n",
      "{'loss': 0.3849, 'learning_rate': 4.655172413793104e-05, 'epoch': 2.37}\n",
      "{'loss': 0.4346, 'learning_rate': 4.3103448275862066e-05, 'epoch': 2.42}\n",
      "{'loss': 0.388, 'learning_rate': 3.965517241379311e-05, 'epoch': 2.47}\n",
      "{'loss': 0.3459, 'learning_rate': 3.620689655172414e-05, 'epoch': 2.51}\n",
      "{'loss': 0.391, 'learning_rate': 3.275862068965517e-05, 'epoch': 2.56}\n",
      "{'loss': 0.5444, 'learning_rate': 2.9310344827586206e-05, 'epoch': 2.6}\n",
      "{'loss': 0.3088, 'learning_rate': 2.5862068965517244e-05, 'epoch': 2.65}\n",
      "{'loss': 0.3706, 'learning_rate': 2.2413793103448276e-05, 'epoch': 2.7}\n",
      "{'loss': 0.2915, 'learning_rate': 1.896551724137931e-05, 'epoch': 2.74}\n",
      "{'loss': 0.3425, 'learning_rate': 1.5517241379310346e-05, 'epoch': 2.79}\n",
      "{'loss': 0.2765, 'learning_rate': 1.206896551724138e-05, 'epoch': 2.84}\n",
      "{'loss': 0.3702, 'learning_rate': 8.620689655172414e-06, 'epoch': 2.88}\n",
      "{'loss': 0.3833, 'learning_rate': 5.172413793103448e-06, 'epoch': 2.93}\n",
      "{'loss': 0.3679, 'learning_rate': 1.724137931034483e-06, 'epoch': 2.98}\n",
      "{'train_runtime': 427.786, 'train_samples_per_second': 24.068, 'train_steps_per_second': 1.508, 'train_loss': 0.6119164782901143, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=645, training_loss=0.6119164782901143, metrics={'train_runtime': 427.786, 'train_samples_per_second': 24.068, 'train_steps_per_second': 1.508, 'train_loss': 0.6119164782901143, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "# Fine-tune the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "model.save_pretrained('/Users/lauraguinandrodriguez/Desktop/Hate Speech')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "model = DistilBertForSequenceClassification.from_pretrained('/Users/lauraguinandrodriguez/Desktop/Hate Speech')\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Function to make predictions\n",
    "def predict(tweet, model, tokenizer):\n",
    "    inputs = tokenizer(tweet, return_tensors=\"pt\", truncation=True, padding=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    probs = outputs.logits.softmax(dim=-1)\n",
    "    return torch.argmax(probs, dim=-1).item()\n",
    "\n",
    "# Make predictions\n",
    "test_data['predicted_label'] = test_data['tweet'].apply(lambda x: predict(x, model, tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8403263403263403\n",
      "Precision: 0.8399636498876226\n",
      "Recall: 0.8403263403263403\n",
      "F1 Score: 0.839816047944601\n",
      "Confusion Matrix:\n",
      "[[234  37  25]\n",
      " [ 47 239   8]\n",
      " [ 16   4 248]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.79      0.79       296\n",
      "           1       0.85      0.81      0.83       294\n",
      "           2       0.88      0.93      0.90       268\n",
      "\n",
      "    accuracy                           0.84       858\n",
      "   macro avg       0.84      0.84      0.84       858\n",
      "weighted avg       0.84      0.84      0.84       858\n",
      "\n",
      "Matthews Correlation Coefficient: 0.760699081345725\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report, matthews_corrcoef\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_data['label'], test_data['predicted_label'])\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(test_data['label'], test_data['predicted_label'], average='weighted')\n",
    "conf_matrix = confusion_matrix(test_data['label'], test_data['predicted_label'])\n",
    "report = classification_report(test_data['label'], test_data['predicted_label'])\n",
    "mcc = matthews_corrcoef(test_data['label'], test_data['predicted_label'])\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{report}\")\n",
    "print(f\"Matthews Correlation Coefficient: {mcc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming 'hatespeech['label']' are the true labels and 'hatespeech['predicted_label']' are the predicted labels\n",
    "true_labels = test_data['label']\n",
    "predicted_labels = test_data['predicted_label']\n",
    "\n",
    "# Generating the confusion matrix\n",
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Labels for the classes\n",
    "class_names = ['Hate Speech', 'Offensive Language', 'Neither']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face existing pre-trained hate-speech model\n",
    "#### Source: Sakil/distilbert_lazylearner_hatespeech_detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the pipeline with the DistilBERT model\n",
    "pipe = pipeline(\"text-classification\", model=\"Sakil/distilbert_lazylearner_hatespeech_detection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get predictions for the entire dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhatespeech\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtweet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtolist\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:156\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    123\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;03m    Classify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m        If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/base.py:1121\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1118\u001b[0m     final_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[1;32m   1119\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1120\u001b[0m     )\n\u001b[0;32m-> 1121\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfinal_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;66;03m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:187\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    186\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:779\u001b[0m, in \u001b[0;36mDistilBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    771\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    773\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    774\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    775\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    777\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m--> 779\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    780\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    781\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    784\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    788\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[1;32m    789\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m hidden_state[:, \u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:599\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    595\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    597\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(input_ids, inputs_embeds)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m--> 599\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:369\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    361\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    362\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    363\u001b[0m         hidden_state,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m         output_attentions,\n\u001b[1;32m    367\u001b[0m     )\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 369\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:313\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    310\u001b[0m sa_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa_layer_norm(sa_output \u001b[38;5;241m+\u001b[39m x)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;66;03m# Feed Forward Network\u001b[39;00m\n\u001b[0;32m--> 313\u001b[0m ffn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa_output\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    314\u001b[0m ffn_output: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_layer_norm(ffn_output \u001b[38;5;241m+\u001b[39m sa_output)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[1;32m    316\u001b[0m output \u001b[38;5;241m=\u001b[39m (ffn_output,)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:254\u001b[0m, in \u001b[0;36mFFN.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 254\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mff_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/pytorch_utils.py:241\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/transformers/models/distilbert/modeling_distilbert.py:259\u001b[0m, in \u001b[0;36mFFN.ff_chunk\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    257\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlin1(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    258\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation(x)\n\u001b[0;32m--> 259\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get predictions for the entire dataset\n",
    "predictions = pipe(hatespeech['tweet'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: no one goes into psychiatric to help people. those people are all immoral. everyone who works in that field is pure trash.\n",
      "Prediction: {'label': 'LABEL_1', 'score': 0.8438279032707214}\n",
      "\n",
      "Tweet: it  us ghettos-driven by liberal ideology; forged by unions &amp; bankruptcy by self-serving gems watering to parasites!trot \n",
      "Prediction: {'label': 'LABEL_1', 'score': 0.877532958984375}\n",
      "\n",
      "Tweet: i witty wrecked a bitch.\n",
      "Prediction: {'label': 'LABEL_1', 'score': 0.7545727491378784}\n",
      "\n",
      "Tweet: oh no she didn &128129; &8220; watch to mouth bitch \" one bitch and two dogs \n",
      "Prediction: {'label': 'LABEL_1', 'score': 0.786658763885498}\n",
      "\n",
      "Tweet: it  y'all birches be like 16 looking 25 &128553;\n",
      "Prediction: {'label': 'LABEL_1', 'score': 0.8328980207443237}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sample data for demonstration\n",
    "sample_data = hatespeech['tweet'].sample(n=5, random_state=1)\n",
    "sample_predictions = pipe(sample_data.tolist())\n",
    "\n",
    "for tweet, prediction in zip(sample_data, sample_predictions):\n",
    "    print(f\"Tweet: {tweet}\\nPrediction: {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your model outputs string labels like 'LABEL_0', 'LABEL_1', 'LABEL_2'\n",
    "label_mapping = {'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2}\n",
    "predicted_labels_numeric = [label_mapping[label['label']] for label in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.34265734265734266\n",
      "Precision: 0.23109681250360048\n",
      "Recall: 0.34265734265734266\n",
      "F1 Score: 0.2758571680949288\n",
      "Confusion Matrix:\n",
      "[[ 610  820    0]\n",
      " [ 570  860    0]\n",
      " [1077  353    0]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.43      0.33      1430\n",
      "           1       0.42      0.60      0.50      1430\n",
      "           2       0.00      0.00      0.00      1430\n",
      "\n",
      "    accuracy                           0.34      4290\n",
      "   macro avg       0.23      0.34      0.28      4290\n",
      "weighted avg       0.23      0.34      0.28      4290\n",
      "\n",
      "Matthews Correlation Coefficient: 0.016171717798939125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Calculate metrics using the numeric predicted labels\n",
    "accuracy = accuracy_score(hatespeech['label'], predicted_labels_numeric)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(hatespeech['label'], predicted_labels_numeric, average='weighted')\n",
    "conf_matrix = confusion_matrix(hatespeech['label'], predicted_labels_numeric)\n",
    "report = classification_report(hatespeech['label'], predicted_labels_numeric)\n",
    "mcc = matthews_corrcoef(hatespeech['label'], predicted_labels_numeric)\n",
    "\n",
    "# Printing the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Classification Report:\\n{report}\")\n",
    "print(f\"Matthews Correlation Coefficient: {mcc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting speechbrain\n",
      "  Downloading speechbrain-0.5.16-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting hyperpyyaml (from speechbrain)\n",
      "  Downloading HyperPyYAML-1.2.2-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from speechbrain) (1.2.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from speechbrain) (1.23.5)\n",
      "Requirement already satisfied: packaging in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from speechbrain) (23.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from speechbrain) (1.10.1)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from speechbrain) (0.1.99)\n",
      "Requirement already satisfied: torch>=1.9 in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from speechbrain) (2.1.1)\n",
      "Collecting torchaudio (from speechbrain)\n",
      "  Downloading torchaudio-2.2.0-cp311-cp311-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: tqdm in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from speechbrain) (4.66.1)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from speechbrain) (0.19.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.9->speechbrain) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.9->speechbrain) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.9->speechbrain) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.9->speechbrain) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.9->speechbrain) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from torch>=1.9->speechbrain) (2023.10.0)\n",
      "Requirement already satisfied: requests in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub->speechbrain) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub->speechbrain) (6.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->speechbrain) (0.4.6)\n",
      "Collecting ruamel.yaml>=0.17.28 (from hyperpyyaml->speechbrain)\n",
      "  Downloading ruamel.yaml-0.18.6-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting torch>=1.9 (from speechbrain)\n",
      "  Downloading torch-2.2.0-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain)\n",
      "  Downloading ruamel.yaml.clib-0.2.8-cp311-cp311-win_amd64.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from jinja2->torch>=1.9->speechbrain) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub->speechbrain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub->speechbrain) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub->speechbrain) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from requests->huggingface-hub->speechbrain) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from sympy->torch>=1.9->speechbrain) (1.3.0)\n",
      "Downloading speechbrain-0.5.16-py3-none-any.whl (630 kB)\n",
      "   ---------------------------------------- 630.6/630.6 kB 8.0 MB/s eta 0:00:00\n",
      "Downloading HyperPyYAML-1.2.2-py3-none-any.whl (16 kB)\n",
      "Downloading torchaudio-2.2.0-cp311-cp311-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 2.4/2.4 MB 37.5 MB/s eta 0:00:00\n",
      "Downloading torch-2.2.0-cp311-cp311-win_amd64.whl (198.6 MB)\n",
      "   --------------------------------------- 198.6/198.6 MB 22.6 MB/s eta 0:00:00\n",
      "Downloading ruamel.yaml-0.18.6-py3-none-any.whl (117 kB)\n",
      "   ---------------------------------------- 117.8/117.8 kB 7.2 MB/s eta 0:00:00\n",
      "Downloading ruamel.yaml.clib-0.2.8-cp311-cp311-win_amd64.whl (118 kB)\n",
      "   ---------------------------------------- 118.0/118.0 kB 6.7 MB/s eta 0:00:00\n",
      "Installing collected packages: ruamel.yaml.clib, torch, ruamel.yaml, torchaudio, hyperpyyaml, speechbrain\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.1.1\n",
      "    Uninstalling torch-2.1.1:\n",
      "      Successfully uninstalled torch-2.1.1\n",
      "Successfully installed hyperpyyaml-1.2.2 ruamel.yaml-0.18.6 ruamel.yaml.clib-0.2.8 speechbrain-0.5.16 torch-2.2.0 torchaudio-2.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe and torchrun.exe are installed in 'C:\\Users\\PabloFabianFaundezGa\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.16.1 requires torch==2.1.1, but you have torch 2.2.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install speechbrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PabloFabianFaundezGa\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_vendor\\packaging\\requirements.py\", line 102, in __init__\n",
      "    req = REQUIREMENT.parseString(requirement_string)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PabloFabianFaundezGa\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_vendor\\pyparsing\\util.py\", line 256, in _inner\n",
      "    return fn(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PabloFabianFaundezGa\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_vendor\\pyparsing\\core.py\", line 1190, in parse_string\n",
      "    raise exc.with_traceback(None)\n",
      "pip._vendor.pyparsing.exceptions.ParseException: Expected string_end, found '['  (at char 11), (line:1, col:12)\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\PabloFabianFaundezGa\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 180, in exc_logging_wrapper\n",
      "    status = run_func(*args)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PabloFabianFaundezGa\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 245, in wrapper\n",
      "    return func(self, options, args)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PabloFabianFaundezGa\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\commands\\install.py\", line 342, in run\n",
      "    reqs = self.get_requirements(args, options, finder, session)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PabloFabianFaundezGa\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 411, in get_requirements\n",
      "    req_to_add = install_req_from_line(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PabloFabianFaundezGa\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\req\\constructors.py\", line 421, in install_req_from_line\n",
      "    parts = parse_req_from_line(name, line_source)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PabloFabianFaundezGa\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\req\\constructors.py\", line 358, in parse_req_from_line\n",
      "    extras = convert_extras(extras_as_string)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PabloFabianFaundezGa\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\req\\constructors.py\", line 58, in convert_extras\n",
      "    return get_requirement(\"placeholder\" + extras.lower()).extras\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PabloFabianFaundezGa\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_internal\\utils\\packaging.py\", line 45, in get_requirement\n",
      "    return Requirement(req_string)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\PabloFabianFaundezGa\\AppData\\Roaming\\Python\\Python311\\site-packages\\pip\\_vendor\\packaging\\requirements.py\", line 104, in __init__\n",
      "    raise InvalidRequirement(\n",
      "pip._vendor.packaging.requirements.InvalidRequirement: Parse error at \"\"['all']\"\": Expected string_end\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nemo_toolkit['all']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting git+https://github.com/NVIDIA/NeMo.git\n",
      "  Cloning https://github.com/NVIDIA/NeMo.git to c:\\users\\pablofabianfaundezga\\appdata\\local\\temp\\pip-req-build-yfhpi9ex\n",
      "  Resolved https://github.com/NVIDIA/NeMo.git to commit 8349d63f3210ca6b397ef47f8171bc5452db191c\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting huggingface-hub>=0.20.3 (from nemo_toolkit==1.23.0rc0)\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numba in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from nemo_toolkit==1.23.0rc0) (0.58.1)\n",
      "Requirement already satisfied: numpy>=1.22 in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from nemo_toolkit==1.23.0rc0) (1.23.5)\n",
      "Requirement already satisfied: onnx>=1.7.0 in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from nemo_toolkit==1.23.0rc0) (1.15.0)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from nemo_toolkit==1.23.0rc0) (2.8.2)\n",
      "Requirement already satisfied: ruamel.yaml in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from nemo_toolkit==1.23.0rc0) (0.18.6)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from nemo_toolkit==1.23.0rc0) (1.2.2)\n",
      "Requirement already satisfied: setuptools>=65.5.1 in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from nemo_toolkit==1.23.0rc0) (67.8.0)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from nemo_toolkit==1.23.0rc0) (2.14.1)\n",
      "Requirement already satisfied: text-unidecode in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from nemo_toolkit==1.23.0rc0) (1.3)\n",
      "Requirement already satisfied: torch in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from nemo_toolkit==1.23.0rc0) (2.2.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (from nemo_toolkit==1.23.0rc0) (4.66.1)\n",
      "INFO: pip is looking at multiple versions of nemo-toolkit to determine which version is compatible with other requirements. This could take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/NVIDIA/NeMo.git 'C:\\Users\\PabloFabianFaundezGa\\AppData\\Local\\Temp\\pip-req-build-yfhpi9ex'\n",
      "ERROR: Could not find a version that satisfies the requirement triton (from nemo-toolkit) (from versions: none)\n",
      "ERROR: No matching distribution found for triton\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/NVIDIA/NeMo.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nemo'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Load FastPitch\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnemo\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcollections\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtts\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodels\u001b[39;00m \u001b[39mimport\u001b[39;00m FastPitchModel\n\u001b[0;32m      3\u001b[0m spec_generator \u001b[39m=\u001b[39m FastPitchModel\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mnvidia/tts_en_fastpitch\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39m# Load vocoder\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nemo'"
     ]
    }
   ],
   "source": [
    "# Load FastPitch\n",
    "from nemo.collections.tts.models import FastPitchModel\n",
    "spec_generator = FastPitchModel.from_pretrained(\"nvidia/tts_en_fastpitch\")\n",
    "\n",
    "# Load vocoder\n",
    "from nemo.collections.tts.models import HifiGanModel\n",
    "model = HifiGanModel.from_pretrained(model_name=\"nvidia/tts_hifigan\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: Cython in c:\\users\\pablofabianfaundezga\\appdata\\roaming\\python\\python311\\site-packages (3.0.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install Cython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spec_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msoundfile\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msf\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m parsed \u001b[39m=\u001b[39m spec_generator\u001b[39m.\u001b[39mparse(\u001b[39m\"\u001b[39m\u001b[39mYou can type your sentence here to get nemo to produce speech.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m spectrogram \u001b[39m=\u001b[39m spec_generator\u001b[39m.\u001b[39mgenerate_spectrogram(tokens\u001b[39m=\u001b[39mparsed)\n\u001b[0;32m      4\u001b[0m audio \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mconvert_spectrogram_to_audio(spec\u001b[39m=\u001b[39mspectrogram)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spec_generator' is not defined"
     ]
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "parsed = spec_generator.parse(\"You can type your sentence here to get nemo to produce speech.\")\n",
    "spectrogram = spec_generator.generate_spectrogram(tokens=parsed)\n",
    "audio = model.convert_spectrogram_to_audio(spec=spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
