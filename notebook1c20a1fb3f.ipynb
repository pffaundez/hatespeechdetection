{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Twitter Sentiment Analysis with BERT and roBERTa transformers"]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://i.imgur.com/3tASSmp.jpg\" width=\"700px\">"]},{"cell_type":"markdown","metadata":{},"source":["This project is about the **analysis of tweets about coronavirus**, with the goal of performing a **Sentiment Analysis using BERT and roBERTa** algorithms to predict the emotion of a tweet (Positive, Negative or Neutral). In particular, both **BERT and ROBERTA will be fine tuned** using the given dataset in order to improve the model overall performance.<br>\n","Before feeding the data to the algorithms, **the tweets will be deeply cleaned to remove links, hashtags at the end of the sentences and punctuation** to allow the algorithms to better understand the text and improve the prediction performance."]},{"cell_type":"markdown","metadata":{},"source":["I performed a similar analysis on non-labeled tweets about Omicron Variant using Vader, NLTK, TextBLOB and Flair NLP algorithms at the following link:\n","- https://www.kaggle.com/ludovicocuoghi/how-are-people-reacting-to-omicron-on-twitter"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2021-12-07T16:04:36.962972Z","iopub.status.busy":"2021-12-07T16:04:36.962613Z","iopub.status.idle":"2021-12-07T16:04:36.982915Z","shell.execute_reply":"2021-12-07T16:04:36.982267Z","shell.execute_reply.started":"2021-12-07T16:04:36.962881Z"}},"source":["# Sentiment Analysis Results:"]},{"cell_type":"markdown","metadata":{},"source":["**The two algorithms performed quite well on the dataset, showing F1 and accuracy scores around 90%.**<br> Such high scores can only be achieved by performing a good cleaning of the original data, allowing the algorithms to learn the most from it.<br> **In particular, also a baseline Naive Bayes Classifier model has been trained to perform the sentiment classification, with a resulting accuracy and F1 around 70% (much lower than BERT)**.<br>\n","**The training of BERT and roBERTa took around 11 minutes per epoch (for a total of 4 epochs) on GPU** per algorithm, since both **the transformers parameters (more than 100 million) have been fine tuned** to perform the best on the given dataset. It is possible to train only the last layer of the transformer without fine tuning the other parameters: however, this usually leads to inferior results compared to the fine tuning approach."]},{"cell_type":"markdown","metadata":{},"source":["<img src=\"https://i.imgur.com/htll3Fu.png\" width=\"900px\">"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_kg_hide-input":true,"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2021-12-22T21:15:17.779154Z","iopub.status.busy":"2021-12-22T21:15:17.778826Z","iopub.status.idle":"2021-12-22T21:15:17.789097Z","shell.execute_reply":"2021-12-22T21:15:17.788146Z","shell.execute_reply.started":"2021-12-22T21:15:17.779117Z"},"trusted":true},"outputs":[],"source":["import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:15:17.791721Z","iopub.status.busy":"2021-12-22T21:15:17.791452Z","iopub.status.idle":"2021-12-22T21:15:17.808398Z","shell.execute_reply":"2021-12-22T21:15:17.807697Z","shell.execute_reply.started":"2021-12-22T21:15:17.791678Z"},"trusted":true},"outputs":[],"source":["#general purpose packages\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","#data processing\n","import re, string\n","import emoji\n","import nltk\n","\n","from sklearn import preprocessing\n","from imblearn.over_sampling import RandomOverSampler\n","from sklearn.model_selection import train_test_split\n","\n","\n","#Naive Bayes\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.naive_bayes import MultinomialNB\n","\n","#transformers\n","from transformers import BertTokenizerFast\n","from transformers import TFBertModel\n","from transformers import RobertaTokenizerFast\n","from transformers import TFRobertaModel\n","\n","#keras\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","\n","#metrics\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","#set seed for reproducibility\n","seed=42\n","\n","#set style for plots\n","sns.set_style(\"whitegrid\")\n","sns.despine()\n","plt.style.use(\"seaborn-whitegrid\")\n","plt.rc(\"figure\", autolayout=True)\n","plt.rc(\"axes\", labelweight=\"bold\", labelsize=\"large\", titleweight=\"bold\", titlepad=10)\n"]},{"cell_type":"markdown","metadata":{},"source":["## Custom functions definition:"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-12-22T21:15:17.810284Z","iopub.status.busy":"2021-12-22T21:15:17.809938Z","iopub.status.idle":"2021-12-22T21:15:17.817316Z","shell.execute_reply":"2021-12-22T21:15:17.816401Z","shell.execute_reply.started":"2021-12-22T21:15:17.810234Z"},"trusted":true},"outputs":[],"source":["def conf_matrix(y, y_pred, title):\n","    fig, ax =plt.subplots(figsize=(5,5))\n","    labels=['Negative', 'Neutral', 'Positive']\n","    ax=sns.heatmap(confusion_matrix(y, y_pred), annot=True, cmap=\"Blues\", fmt='g', cbar=False, annot_kws={\"size\":25})\n","    plt.title(title, fontsize=20)\n","    ax.xaxis.set_ticklabels(labels, fontsize=17) \n","    ax.yaxis.set_ticklabels(labels, fontsize=17)\n","    ax.set_ylabel('Test', fontsize=20)\n","    ax.set_xlabel('Predicted', fontsize=20)\n","    plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["# Loading the data"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:15:17.819093Z","iopub.status.busy":"2021-12-22T21:15:17.818782Z","iopub.status.idle":"2021-12-22T21:15:17.964962Z","shell.execute_reply":"2021-12-22T21:15:17.964233Z","shell.execute_reply.started":"2021-12-22T21:15:17.819059Z"},"trusted":true},"outputs":[],"source":["df = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_train.csv',encoding='ISO-8859-1')\n","df_test = pd.read_csv('../input/covid-19-nlp-text-classification/Corona_NLP_test.csv')"]},{"cell_type":"markdown","metadata":{},"source":["NOTE: UTF-8 encoding does not work on the dataset when loading it with pandas 'read_csv' function. This lead to the use of 'ISO-8859-1'/latin-1 encoding. <br>\n","It will be found later that some special characters like apostrophes are turned into '\\x92', which will be taken care of during the data cleaning process.\n","                                                                                                         "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:15:17.967557Z","iopub.status.busy":"2021-12-22T21:15:17.967262Z","iopub.status.idle":"2021-12-22T21:15:17.979179Z","shell.execute_reply":"2021-12-22T21:15:17.978344Z","shell.execute_reply.started":"2021-12-22T21:15:17.967519Z"},"trusted":true},"outputs":[],"source":["df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:15:17.981042Z","iopub.status.busy":"2021-12-22T21:15:17.980745Z","iopub.status.idle":"2021-12-22T21:15:18.011269Z","shell.execute_reply":"2021-12-22T21:15:18.010543Z","shell.execute_reply.started":"2021-12-22T21:15:17.981005Z"},"trusted":true},"outputs":[],"source":["df.info()"]},{"cell_type":"markdown","metadata":{},"source":["We convert the date column 'TweetAt' to pandas datetime format to improve its usability in the further analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:15:18.012651Z","iopub.status.busy":"2021-12-22T21:15:18.012419Z","iopub.status.idle":"2021-12-22T21:15:18.025611Z","shell.execute_reply":"2021-12-22T21:15:18.024835Z","shell.execute_reply.started":"2021-12-22T21:15:18.012618Z"},"trusted":true},"outputs":[],"source":["df['TweetAt'] = pd.to_datetime(df['TweetAt'])"]},{"cell_type":"markdown","metadata":{},"source":["## Duplicate tweets?"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:15:18.027027Z","iopub.status.busy":"2021-12-22T21:15:18.026722Z","iopub.status.idle":"2021-12-22T21:15:18.067792Z","shell.execute_reply":"2021-12-22T21:15:18.067127Z","shell.execute_reply.started":"2021-12-22T21:15:18.026991Z"},"trusted":true},"outputs":[],"source":["df.drop_duplicates(subset='OriginalTweet',inplace=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:15:18.069285Z","iopub.status.busy":"2021-12-22T21:15:18.069027Z","iopub.status.idle":"2021-12-22T21:15:18.094153Z","shell.execute_reply":"2021-12-22T21:15:18.092972Z","shell.execute_reply.started":"2021-12-22T21:15:18.069251Z"},"trusted":true},"outputs":[],"source":["df.info()"]},{"cell_type":"markdown","metadata":{},"source":["Good news, No duplicate tweets !"]},{"cell_type":"markdown","metadata":{},"source":["# Tweets count by date"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-12-22T21:15:18.097078Z","iopub.status.busy":"2021-12-22T21:15:18.096834Z","iopub.status.idle":"2021-12-22T21:15:18.380865Z","shell.execute_reply":"2021-12-22T21:15:18.380184Z","shell.execute_reply.started":"2021-12-22T21:15:18.097047Z"},"trusted":true},"outputs":[],"source":["tweets_per_day = df['TweetAt'].dt.strftime('%m-%d').value_counts().sort_index().reset_index(name='counts')"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-12-22T21:15:18.383812Z","iopub.status.busy":"2021-12-22T21:15:18.383206Z","iopub.status.idle":"2021-12-22T21:15:19.037172Z","shell.execute_reply":"2021-12-22T21:15:19.036514Z","shell.execute_reply.started":"2021-12-22T21:15:18.383781Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(20,5))\n","ax = sns.barplot(x='index', y='counts', data=tweets_per_day,edgecolor = 'black',ci=False, palette='Blues_r')\n","plt.title('Tweets count by date')\n","plt.yticks([])\n","ax.bar_label(ax.containers[0])\n","plt.ylabel('count')\n","plt.xlabel('')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We notice that in the dataset there are some days without tweets in the dataset. Among the days with tweets, most of them are made around the end of March: from 18th of Match to the 26th of March."]},{"cell_type":"markdown","metadata":{},"source":["# Tweets per country and city"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-12-22T21:15:19.038736Z","iopub.status.busy":"2021-12-22T21:15:19.038417Z","iopub.status.idle":"2021-12-22T21:15:19.054194Z","shell.execute_reply":"2021-12-22T21:15:19.053557Z","shell.execute_reply.started":"2021-12-22T21:15:19.038697Z"},"trusted":true},"outputs":[],"source":["tweets_per_country = df['Location'].value_counts().loc[lambda x : x > 100].reset_index(name='counts')"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-12-22T21:15:19.05591Z","iopub.status.busy":"2021-12-22T21:15:19.055623Z","iopub.status.idle":"2021-12-22T21:15:19.664941Z","shell.execute_reply":"2021-12-22T21:15:19.664287Z","shell.execute_reply.started":"2021-12-22T21:15:19.055874Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(15,6))\n","ax = sns.barplot(x='index', y='counts', data=tweets_per_country,edgecolor = 'black',ci=False, palette='Spectral')\n","plt.title('Tweets count by country')\n","plt.xticks(rotation=70)\n","plt.yticks([])\n","ax.bar_label(ax.containers[0])\n","plt.ylabel('count')\n","plt.xlabel('')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["The 'location' column contains both countries and cities. It could be interesting to separate cities and countries, however this wont be investigated in this work."]},{"cell_type":"markdown","metadata":{},"source":["# Tweets Deep Cleaning"]},{"cell_type":"markdown","metadata":{},"source":["In the following, we will perform some data cleaning on the raw text of the tweets.<br>\n","To simplify the analaysis, we will just keep the columns 'Originaltweet' (raw tweets) and the target column 'Sentiment'."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:15:19.666889Z","iopub.status.busy":"2021-12-22T21:15:19.666416Z","iopub.status.idle":"2021-12-22T21:15:19.674072Z","shell.execute_reply":"2021-12-22T21:15:19.673268Z","shell.execute_reply.started":"2021-12-22T21:15:19.66685Z"},"trusted":true},"outputs":[],"source":["df = df[['OriginalTweet','Sentiment']]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:15:19.675834Z","iopub.status.busy":"2021-12-22T21:15:19.675524Z","iopub.status.idle":"2021-12-22T21:15:19.685079Z","shell.execute_reply":"2021-12-22T21:15:19.684432Z","shell.execute_reply.started":"2021-12-22T21:15:19.675777Z"},"trusted":true},"outputs":[],"source":["df_test = df_test[['OriginalTweet','Sentiment']]"]},{"cell_type":"markdown","metadata":{},"source":["**Then we define custom functions to clean the text of the tweets.**"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:15:19.687746Z","iopub.status.busy":"2021-12-22T21:15:19.687068Z","iopub.status.idle":"2021-12-22T21:15:19.698296Z","shell.execute_reply":"2021-12-22T21:15:19.697572Z","shell.execute_reply.started":"2021-12-22T21:15:19.687704Z"},"trusted":true},"outputs":[],"source":["##CUSTOM DEFINED FUNCTIONS TO CLEAN THE TWEETS\n","\n","#Clean emojis from text\n","def strip_emoji(text):\n","    return re.sub(emoji.get_emoji_regexp(), r\"\", text) #remove emoji\n","\n","#Remove punctuations, links, mentions and \\r\\n new line characters\n","def strip_all_entities(text): \n","    text = text.replace('\\r', '').replace('\\n', ' ').replace('\\n', ' ').lower() #remove \\n and \\r and lowercase\n","    text = re.sub(r\"(?:\\@|https?\\://)\\S+\", \"\", text) #remove links and mentions\n","    text = re.sub(r'[^\\x00-\\x7f]',r'', text) #remove non utf8/ascii characters such as '\\x9a\\x91\\x97\\x9a\\x97'\n","    banned_list= string.punctuation + 'Ã'+'±'+'ã'+'¼'+'â'+'»'+'§'\n","    table = str.maketrans('', '', banned_list)\n","    text = text.translate(table)\n","    return text\n","\n","#clean hashtags at the end of the sentence, and keep those in the middle of the sentence by removing just the # symbol\n","def clean_hashtags(tweet):\n","    new_tweet = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet)) #remove last hashtags\n","    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet)) #remove hashtags symbol from words in the middle of the sentence\n","    return new_tweet2\n","\n","#Filter special characters such as & and $ present in some words\n","def filter_chars(a):\n","    sent = []\n","    for word in a.split(' '):\n","        if ('$' in word) | ('&' in word):\n","            sent.append('')\n","        else:\n","            sent.append(word)\n","    return ' '.join(sent)\n","\n","def remove_mult_spaces(text): # remove multiple spaces\n","    return re.sub(\"\\s\\s+\" , \" \", text)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:15:19.701235Z","iopub.status.busy":"2021-12-22T21:15:19.701015Z","iopub.status.idle":"2021-12-22T21:16:17.869828Z","shell.execute_reply":"2021-12-22T21:16:17.869039Z","shell.execute_reply.started":"2021-12-22T21:15:19.701201Z"},"trusted":true},"outputs":[],"source":["texts_new = []\n","for t in df.OriginalTweet:\n","    texts_new.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(strip_emoji(t))))))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:17.871709Z","iopub.status.busy":"2021-12-22T21:16:17.871222Z","iopub.status.idle":"2021-12-22T21:16:23.197999Z","shell.execute_reply":"2021-12-22T21:16:23.197295Z","shell.execute_reply.started":"2021-12-22T21:16:17.871667Z"},"trusted":true},"outputs":[],"source":["texts_new_test = []\n","for t in df_test.OriginalTweet:\n","    texts_new_test.append(remove_mult_spaces(filter_chars(clean_hashtags(strip_all_entities(strip_emoji(t))))))"]},{"cell_type":"markdown","metadata":{},"source":["Now we can create a new column, for both train and test sets, to host the cleaned version of the tweets' text."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:23.199648Z","iopub.status.busy":"2021-12-22T21:16:23.199371Z","iopub.status.idle":"2021-12-22T21:16:23.212118Z","shell.execute_reply":"2021-12-22T21:16:23.211257Z","shell.execute_reply.started":"2021-12-22T21:16:23.199614Z"},"trusted":true},"outputs":[],"source":["df['text_clean'] = texts_new\n","df_test['text_clean'] = texts_new_test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:23.213745Z","iopub.status.busy":"2021-12-22T21:16:23.213498Z","iopub.status.idle":"2021-12-22T21:16:23.227327Z","shell.execute_reply":"2021-12-22T21:16:23.226641Z","shell.execute_reply.started":"2021-12-22T21:16:23.213708Z"},"trusted":true},"outputs":[],"source":["df['text_clean'].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:23.232676Z","iopub.status.busy":"2021-12-22T21:16:23.232295Z","iopub.status.idle":"2021-12-22T21:16:23.239727Z","shell.execute_reply":"2021-12-22T21:16:23.239023Z","shell.execute_reply.started":"2021-12-22T21:16:23.232647Z"},"trusted":true},"outputs":[],"source":["df_test['text_clean'].head()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:23.241499Z","iopub.status.busy":"2021-12-22T21:16:23.241001Z","iopub.status.idle":"2021-12-22T21:16:23.250236Z","shell.execute_reply":"2021-12-22T21:16:23.249487Z","shell.execute_reply.started":"2021-12-22T21:16:23.241461Z"},"trusted":true},"outputs":[],"source":["df['text_clean'][1:8].values"]},{"cell_type":"markdown","metadata":{},"source":["Moreover, we will also create a column to host the lenght of the cleaned text, to check if by cleaning the text we removed too much text or almost entirely the tweet!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:23.253023Z","iopub.status.busy":"2021-12-22T21:16:23.252435Z","iopub.status.idle":"2021-12-22T21:16:23.346853Z","shell.execute_reply":"2021-12-22T21:16:23.346251Z","shell.execute_reply.started":"2021-12-22T21:16:23.252986Z"},"trusted":true},"outputs":[],"source":["text_len = []\n","for text in df.text_clean:\n","    tweet_len = len(text.split())\n","    text_len.append(tweet_len)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:23.348129Z","iopub.status.busy":"2021-12-22T21:16:23.347901Z","iopub.status.idle":"2021-12-22T21:16:23.369365Z","shell.execute_reply":"2021-12-22T21:16:23.368729Z","shell.execute_reply.started":"2021-12-22T21:16:23.348096Z"},"trusted":true},"outputs":[],"source":["df['text_len'] = text_len"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:23.371086Z","iopub.status.busy":"2021-12-22T21:16:23.370787Z","iopub.status.idle":"2021-12-22T21:16:23.387455Z","shell.execute_reply":"2021-12-22T21:16:23.386622Z","shell.execute_reply.started":"2021-12-22T21:16:23.371047Z"},"trusted":true},"outputs":[],"source":["text_len_test = []\n","for text in df_test.text_clean:\n","    tweet_len = len(text.split())\n","    text_len_test.append(tweet_len)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:23.38944Z","iopub.status.busy":"2021-12-22T21:16:23.388853Z","iopub.status.idle":"2021-12-22T21:16:23.398823Z","shell.execute_reply":"2021-12-22T21:16:23.398101Z","shell.execute_reply.started":"2021-12-22T21:16:23.389402Z"},"trusted":true},"outputs":[],"source":["df_test['text_len'] = text_len_test"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-12-22T21:16:23.400721Z","iopub.status.busy":"2021-12-22T21:16:23.400127Z","iopub.status.idle":"2021-12-22T21:16:23.670878Z","shell.execute_reply":"2021-12-22T21:16:23.67021Z","shell.execute_reply.started":"2021-12-22T21:16:23.400676Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(7,5))\n","ax = sns.countplot(x='text_len', data=df[df['text_len']<10], palette='mako')\n","plt.title('Training tweets with less than 10 words')\n","plt.yticks([])\n","ax.bar_label(ax.containers[0])\n","plt.ylabel('count')\n","plt.xlabel('')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-12-22T21:16:23.672665Z","iopub.status.busy":"2021-12-22T21:16:23.672187Z","iopub.status.idle":"2021-12-22T21:16:23.926009Z","shell.execute_reply":"2021-12-22T21:16:23.925218Z","shell.execute_reply.started":"2021-12-22T21:16:23.672624Z"},"trusted":true},"outputs":[],"source":["plt.figure(figsize=(7,5))\n","ax = sns.countplot(x='text_len', data=df_test[df_test['text_len']<10], palette='mako')\n","plt.title('Test tweets with less than 10 words')\n","plt.yticks([])\n","ax.bar_label(ax.containers[0])\n","plt.ylabel('count')\n","plt.xlabel('')\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["As we can see, there are lots of cleaned tweets with 0 words: this is due to the cleaning performed before. This means that some tweets contained only mentions, hashtags and links, which have been removed. We will drop these empty tweets and also those with less than 5 words."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:23.927515Z","iopub.status.busy":"2021-12-22T21:16:23.927254Z","iopub.status.idle":"2021-12-22T21:16:23.935987Z","shell.execute_reply":"2021-12-22T21:16:23.935269Z","shell.execute_reply.started":"2021-12-22T21:16:23.92748Z"},"trusted":true},"outputs":[],"source":["print(f\" DF SHAPE: {df.shape}\")\n","print(f\" DF TEST SHAPE: {df_test.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:23.937742Z","iopub.status.busy":"2021-12-22T21:16:23.937463Z","iopub.status.idle":"2021-12-22T21:16:23.947934Z","shell.execute_reply":"2021-12-22T21:16:23.947188Z","shell.execute_reply.started":"2021-12-22T21:16:23.937695Z"},"trusted":true},"outputs":[],"source":["df = df[df['text_len'] > 4]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:23.949722Z","iopub.status.busy":"2021-12-22T21:16:23.949457Z","iopub.status.idle":"2021-12-22T21:16:23.956516Z","shell.execute_reply":"2021-12-22T21:16:23.955817Z","shell.execute_reply.started":"2021-12-22T21:16:23.949679Z"},"trusted":true},"outputs":[],"source":["df_test = df_test[df_test['text_len'] > 4]"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:23.958147Z","iopub.status.busy":"2021-12-22T21:16:23.957889Z","iopub.status.idle":"2021-12-22T21:16:23.965132Z","shell.execute_reply":"2021-12-22T21:16:23.964401Z","shell.execute_reply.started":"2021-12-22T21:16:23.958113Z"},"trusted":true},"outputs":[],"source":["print(f\" DF SHAPE: {df.shape}\")\n","print(f\" DF TEST SHAPE: {df_test.shape}\")"]},{"cell_type":"markdown","metadata":{},"source":["## Training data deeper cleaning"]},{"cell_type":"markdown","metadata":{},"source":["Let's perform a further cleaning checking the tokenizer version of the sentences."]},{"cell_type":"markdown","metadata":{},"source":["First, we import the BERT tokenizer."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2021-12-22T21:16:23.966874Z","iopub.status.busy":"2021-12-22T21:16:23.966349Z","iopub.status.idle":"2021-12-22T21:16:30.505565Z","shell.execute_reply":"2021-12-22T21:16:30.504847Z","shell.execute_reply.started":"2021-12-22T21:16:23.966837Z"},"trusted":true},"outputs":[],"source":["tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:30.507117Z","iopub.status.busy":"2021-12-22T21:16:30.506822Z","iopub.status.idle":"2021-12-22T21:16:38.608135Z","shell.execute_reply":"2021-12-22T21:16:38.60725Z","shell.execute_reply.started":"2021-12-22T21:16:30.507082Z"},"trusted":true},"outputs":[],"source":["token_lens = []\n","\n","for txt in df['text_clean'].values:\n","    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n","    token_lens.append(len(tokens))\n","    \n","max_len=np.max(token_lens)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:38.614497Z","iopub.status.busy":"2021-12-22T21:16:38.612489Z","iopub.status.idle":"2021-12-22T21:16:38.622543Z","shell.execute_reply":"2021-12-22T21:16:38.621788Z","shell.execute_reply.started":"2021-12-22T21:16:38.614458Z"},"trusted":true},"outputs":[],"source":["print(f\"MAX TOKENIZED SENTENCE LENGTH: {max_len}\")"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2021-12-07T12:49:48.842301Z","iopub.status.busy":"2021-12-07T12:49:48.842045Z","iopub.status.idle":"2021-12-07T12:49:48.848445Z","shell.execute_reply":"2021-12-07T12:49:48.847098Z","shell.execute_reply.started":"2021-12-07T12:49:48.842267Z"}},"source":["Let's check the long tokenized sentences (with more than 80 tokens ):"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:38.627927Z","iopub.status.busy":"2021-12-22T21:16:38.627275Z","iopub.status.idle":"2021-12-22T21:16:45.918973Z","shell.execute_reply":"2021-12-22T21:16:45.918256Z","shell.execute_reply.started":"2021-12-22T21:16:38.62789Z"},"trusted":true},"outputs":[],"source":["token_lens = []\n","\n","for i,txt in enumerate(df['text_clean'].values):\n","    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n","    token_lens.append(len(tokens))\n","    if len(tokens)>80:\n","        print(f\"INDEX: {i}, TEXT: {txt}\")  "]},{"cell_type":"markdown","metadata":{},"source":["These sentences are not in english. They should be dropped."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:45.920514Z","iopub.status.busy":"2021-12-22T21:16:45.920127Z","iopub.status.idle":"2021-12-22T21:16:45.942482Z","shell.execute_reply":"2021-12-22T21:16:45.941845Z","shell.execute_reply.started":"2021-12-22T21:16:45.920472Z"},"trusted":true},"outputs":[],"source":["df['token_lens'] = token_lens"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:45.943812Z","iopub.status.busy":"2021-12-22T21:16:45.943569Z","iopub.status.idle":"2021-12-22T21:16:45.966583Z","shell.execute_reply":"2021-12-22T21:16:45.965943Z","shell.execute_reply.started":"2021-12-22T21:16:45.943779Z"},"trusted":true},"outputs":[],"source":["df = df.sort_values(by='token_lens', ascending=False)\n","df.head(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:45.968414Z","iopub.status.busy":"2021-12-22T21:16:45.967696Z","iopub.status.idle":"2021-12-22T21:16:45.980023Z","shell.execute_reply":"2021-12-22T21:16:45.97925Z","shell.execute_reply.started":"2021-12-22T21:16:45.968366Z"},"trusted":true},"outputs":[],"source":["df = df.iloc[12:]\n","df.head()"]},{"cell_type":"markdown","metadata":{},"source":["The dataset looks more clean now. We will shuffle it and reset the index."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:45.98155Z","iopub.status.busy":"2021-12-22T21:16:45.981284Z","iopub.status.idle":"2021-12-22T21:16:45.995008Z","shell.execute_reply":"2021-12-22T21:16:45.994358Z","shell.execute_reply.started":"2021-12-22T21:16:45.981515Z"},"trusted":true},"outputs":[],"source":["df = df.sample(frac=1).reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{},"source":["## Test data deeper cleaning"]},{"cell_type":"markdown","metadata":{},"source":["We will perform the data cleaning based on the tokenized sentences on the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:45.996503Z","iopub.status.busy":"2021-12-22T21:16:45.996245Z","iopub.status.idle":"2021-12-22T21:16:46.749676Z","shell.execute_reply":"2021-12-22T21:16:46.748944Z","shell.execute_reply.started":"2021-12-22T21:16:45.996467Z"},"trusted":true},"outputs":[],"source":["token_lens_test = []\n","\n","for txt in df_test['text_clean'].values:\n","    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n","    token_lens_test.append(len(tokens))\n","    \n","max_len=np.max(token_lens_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:46.751349Z","iopub.status.busy":"2021-12-22T21:16:46.75107Z","iopub.status.idle":"2021-12-22T21:16:46.756497Z","shell.execute_reply":"2021-12-22T21:16:46.755793Z","shell.execute_reply.started":"2021-12-22T21:16:46.751312Z"},"trusted":true},"outputs":[],"source":["print(f\"MAX TOKENIZED SENTENCE LENGTH: {max_len}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:46.758342Z","iopub.status.busy":"2021-12-22T21:16:46.757831Z","iopub.status.idle":"2021-12-22T21:16:47.500654Z","shell.execute_reply":"2021-12-22T21:16:47.499983Z","shell.execute_reply.started":"2021-12-22T21:16:46.758304Z"},"trusted":true},"outputs":[],"source":["token_lens_test = []\n","\n","for i,txt in enumerate(df_test['text_clean'].values):\n","    tokens = tokenizer.encode(txt, max_length=512, truncation=True)\n","    token_lens_test.append(len(tokens))\n","    if len(tokens)>80:\n","        print(f\"INDEX: {i}, TEXT: {txt}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:47.50205Z","iopub.status.busy":"2021-12-22T21:16:47.501809Z","iopub.status.idle":"2021-12-22T21:16:47.508688Z","shell.execute_reply":"2021-12-22T21:16:47.50766Z","shell.execute_reply.started":"2021-12-22T21:16:47.502016Z"},"trusted":true},"outputs":[],"source":["df_test['token_lens'] = token_lens_test"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:47.510349Z","iopub.status.busy":"2021-12-22T21:16:47.509929Z","iopub.status.idle":"2021-12-22T21:16:47.527481Z","shell.execute_reply":"2021-12-22T21:16:47.526777Z","shell.execute_reply.started":"2021-12-22T21:16:47.510311Z"},"trusted":true},"outputs":[],"source":["df_test = df_test.sort_values(by='token_lens', ascending=False)\n","df_test.head(10) "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:47.529191Z","iopub.status.busy":"2021-12-22T21:16:47.528747Z","iopub.status.idle":"2021-12-22T21:16:47.539778Z","shell.execute_reply":"2021-12-22T21:16:47.539121Z","shell.execute_reply.started":"2021-12-22T21:16:47.529156Z"},"trusted":true},"outputs":[],"source":["df_test = df_test.iloc[5:]\n","df_test.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:47.541731Z","iopub.status.busy":"2021-12-22T21:16:47.541138Z","iopub.status.idle":"2021-12-22T21:16:47.548553Z","shell.execute_reply":"2021-12-22T21:16:47.547872Z","shell.execute_reply.started":"2021-12-22T21:16:47.541662Z"},"trusted":true},"outputs":[],"source":["df_test = df_test.sample(frac=1).reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{},"source":["Now the data cleaning is completed. I will perform more data cleaning if I have new ideas !! :)"]},{"cell_type":"markdown","metadata":{},"source":["# Sentiment column analysis"]},{"cell_type":"markdown","metadata":{},"source":["Now we will look at the target column 'Sentiment'."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:47.550374Z","iopub.status.busy":"2021-12-22T21:16:47.549868Z","iopub.status.idle":"2021-12-22T21:16:47.564332Z","shell.execute_reply":"2021-12-22T21:16:47.56366Z","shell.execute_reply.started":"2021-12-22T21:16:47.550336Z"},"trusted":true},"outputs":[],"source":["df['Sentiment'].value_counts()"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2021-12-05T11:35:57.748569Z","iopub.status.busy":"2021-12-05T11:35:57.747932Z","iopub.status.idle":"2021-12-05T11:35:57.753489Z","shell.execute_reply":"2021-12-05T11:35:57.752685Z","shell.execute_reply.started":"2021-12-05T11:35:57.748528Z"}},"source":["The first thing we can do is to encode the categories with numbers. We will also create just 3 possible emotions: Positive, Neutral and Negative."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:47.566009Z","iopub.status.busy":"2021-12-22T21:16:47.565714Z","iopub.status.idle":"2021-12-22T21:16:47.580459Z","shell.execute_reply":"2021-12-22T21:16:47.579684Z","shell.execute_reply.started":"2021-12-22T21:16:47.565975Z"},"trusted":true},"outputs":[],"source":["df['Sentiment'] = df['Sentiment'].map({'Extremely Negative':0,'Negative':0,'Neutral':1,'Positive':2,'Extremely Positive':2})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:47.582551Z","iopub.status.busy":"2021-12-22T21:16:47.581764Z","iopub.status.idle":"2021-12-22T21:16:47.589144Z","shell.execute_reply":"2021-12-22T21:16:47.588501Z","shell.execute_reply.started":"2021-12-22T21:16:47.582474Z"},"trusted":true},"outputs":[],"source":["df_test['Sentiment'] = df_test['Sentiment'].map({'Extremely Negative':0,'Negative':0,'Neutral':1,'Positive':2,'Extremely Positive':2})"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:47.592996Z","iopub.status.busy":"2021-12-22T21:16:47.591248Z","iopub.status.idle":"2021-12-22T21:16:47.599802Z","shell.execute_reply":"2021-12-22T21:16:47.59916Z","shell.execute_reply.started":"2021-12-22T21:16:47.592966Z"},"trusted":true},"outputs":[],"source":["df['Sentiment'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["We note that the three classes are imbalanced. We will proceed with oversampling the train test, to remove bias towards the majority classes."]},{"cell_type":"markdown","metadata":{},"source":["## Class Balancing by RandomOverSampler"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:47.601752Z","iopub.status.busy":"2021-12-22T21:16:47.601159Z","iopub.status.idle":"2021-12-22T21:16:47.697589Z","shell.execute_reply":"2021-12-22T21:16:47.696904Z","shell.execute_reply.started":"2021-12-22T21:16:47.601678Z"},"trusted":true},"outputs":[],"source":["ros = RandomOverSampler()\n","train_x, train_y = ros.fit_resample(np.array(df['text_clean']).reshape(-1, 1), np.array(df['Sentiment']).reshape(-1, 1));\n","train_os = pd.DataFrame(list(zip([x[0] for x in train_x], train_y)), columns = ['text_clean', 'Sentiment']);"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:47.698926Z","iopub.status.busy":"2021-12-22T21:16:47.698691Z","iopub.status.idle":"2021-12-22T21:16:47.705998Z","shell.execute_reply":"2021-12-22T21:16:47.70527Z","shell.execute_reply.started":"2021-12-22T21:16:47.698894Z"},"trusted":true},"outputs":[],"source":["train_os['Sentiment'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["## Train - Validation - Test split"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:47.713316Z","iopub.status.busy":"2021-12-22T21:16:47.713125Z","iopub.status.idle":"2021-12-22T21:16:47.717547Z","shell.execute_reply":"2021-12-22T21:16:47.716647Z","shell.execute_reply.started":"2021-12-22T21:16:47.713294Z"},"trusted":true},"outputs":[],"source":["X = train_os['text_clean'].values\n","y = train_os['Sentiment'].values"]},{"cell_type":"markdown","metadata":{},"source":["A validation set will be extracted from the training set to monitor the validation accuracy, and so prevent overfitting."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:47.719469Z","iopub.status.busy":"2021-12-22T21:16:47.718981Z","iopub.status.idle":"2021-12-22T21:16:47.761536Z","shell.execute_reply":"2021-12-22T21:16:47.760925Z","shell.execute_reply.started":"2021-12-22T21:16:47.719431Z"},"trusted":true},"outputs":[],"source":["X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, stratify=y, random_state=seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:47.763Z","iopub.status.busy":"2021-12-22T21:16:47.762757Z","iopub.status.idle":"2021-12-22T21:16:47.768878Z","shell.execute_reply":"2021-12-22T21:16:47.76812Z","shell.execute_reply.started":"2021-12-22T21:16:47.762967Z"},"trusted":true},"outputs":[],"source":["X_test = df_test['text_clean'].values\n","y_test = df_test['Sentiment'].values"]},{"cell_type":"markdown","metadata":{},"source":["## One hot encoding"]},{"cell_type":"markdown","metadata":{},"source":["After performing some tests, by using one hot encoding on the target variable we achieved higher accuracy. For this reason we will choose one hot enconding over label encoding. <br>\n","EDIT: We will save a copy of the label encoded target columns since they could be useful for further analysis."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:47.770915Z","iopub.status.busy":"2021-12-22T21:16:47.770333Z","iopub.status.idle":"2021-12-22T21:16:47.776791Z","shell.execute_reply":"2021-12-22T21:16:47.776179Z","shell.execute_reply.started":"2021-12-22T21:16:47.770878Z"},"trusted":true},"outputs":[],"source":["y_train_le = y_train.copy()\n","y_valid_le = y_valid.copy()\n","y_test_le = y_test.copy()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:47.778775Z","iopub.status.busy":"2021-12-22T21:16:47.778211Z","iopub.status.idle":"2021-12-22T21:16:47.791468Z","shell.execute_reply":"2021-12-22T21:16:47.790847Z","shell.execute_reply.started":"2021-12-22T21:16:47.778739Z"},"trusted":true},"outputs":[],"source":["ohe = preprocessing.OneHotEncoder()\n","y_train = ohe.fit_transform(np.array(y_train).reshape(-1, 1)).toarray()\n","y_valid = ohe.fit_transform(np.array(y_valid).reshape(-1, 1)).toarray()\n","y_test = ohe.fit_transform(np.array(y_test).reshape(-1, 1)).toarray()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:47.792855Z","iopub.status.busy":"2021-12-22T21:16:47.792606Z","iopub.status.idle":"2021-12-22T21:16:47.797366Z","shell.execute_reply":"2021-12-22T21:16:47.796422Z","shell.execute_reply.started":"2021-12-22T21:16:47.792822Z"},"trusted":true},"outputs":[],"source":["print(f\"TRAINING DATA: {X_train.shape[0]}\\nVALIDATION DATA: {X_valid.shape[0]}\\nTESTING DATA: {X_test.shape[0]}\" )"]},{"cell_type":"markdown","metadata":{},"source":["# Baseline model: Naive Bayes Classifier"]},{"cell_type":"markdown","metadata":{"execution":{"iopub.execute_input":"2021-12-22T19:59:56.68241Z","iopub.status.busy":"2021-12-22T19:59:56.681677Z","iopub.status.idle":"2021-12-22T19:59:56.711837Z","shell.execute_reply":"2021-12-22T19:59:56.71068Z","shell.execute_reply.started":"2021-12-22T19:59:56.6823Z"}},"source":["Before implementing BERT, we will define a simple Naive Bayes baseline model to classify the tweets."]},{"cell_type":"markdown","metadata":{},"source":["First we need to tokenize the tweets using CountVectorizer."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:47.799076Z","iopub.status.busy":"2021-12-22T21:16:47.79882Z","iopub.status.idle":"2021-12-22T21:16:49.887448Z","shell.execute_reply":"2021-12-22T21:16:49.886665Z","shell.execute_reply.started":"2021-12-22T21:16:47.799042Z"},"trusted":true},"outputs":[],"source":["clf = CountVectorizer()\n","X_train_cv =  clf.fit_transform(X_train)\n","X_test_cv = clf.transform(X_test)"]},{"cell_type":"markdown","metadata":{},"source":["Then we create the TF-IDF (term-frequency times inverse document-frequency) versions of the tokenized tweets."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:49.890719Z","iopub.status.busy":"2021-12-22T21:16:49.888669Z","iopub.status.idle":"2021-12-22T21:16:49.984621Z","shell.execute_reply":"2021-12-22T21:16:49.983725Z","shell.execute_reply.started":"2021-12-22T21:16:49.890677Z"},"trusted":true},"outputs":[],"source":["tf_transformer = TfidfTransformer(use_idf=True).fit(X_train_cv)\n","X_train_tf = tf_transformer.transform(X_train_cv)\n","X_test_tf = tf_transformer.transform(X_test_cv)"]},{"cell_type":"markdown","metadata":{},"source":["Now we can define the Naive Bayes Classifier model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:49.986537Z","iopub.status.busy":"2021-12-22T21:16:49.985855Z","iopub.status.idle":"2021-12-22T21:16:49.990851Z","shell.execute_reply":"2021-12-22T21:16:49.989982Z","shell.execute_reply.started":"2021-12-22T21:16:49.986496Z"},"trusted":true},"outputs":[],"source":["nb_clf = MultinomialNB()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:49.992559Z","iopub.status.busy":"2021-12-22T21:16:49.992138Z","iopub.status.idle":"2021-12-22T21:16:50.02363Z","shell.execute_reply":"2021-12-22T21:16:50.022914Z","shell.execute_reply.started":"2021-12-22T21:16:49.992487Z"},"trusted":true},"outputs":[],"source":["nb_clf.fit(X_train_tf, y_train_le)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:50.0258Z","iopub.status.busy":"2021-12-22T21:16:50.025595Z","iopub.status.idle":"2021-12-22T21:16:50.033226Z","shell.execute_reply":"2021-12-22T21:16:50.032277Z","shell.execute_reply.started":"2021-12-22T21:16:50.025776Z"},"trusted":true},"outputs":[],"source":["nb_pred = nb_clf.predict(X_test_tf)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-12-22T21:16:50.035198Z","iopub.status.busy":"2021-12-22T21:16:50.034928Z","iopub.status.idle":"2021-12-22T21:16:50.051762Z","shell.execute_reply":"2021-12-22T21:16:50.051035Z","shell.execute_reply.started":"2021-12-22T21:16:50.035146Z"},"trusted":true},"outputs":[],"source":["print('\\tClassification Report for Naive Bayes:\\n\\n',classification_report(y_test_le,nb_pred, target_names=['Negative', 'Neutral', 'Positive']))"]},{"cell_type":"markdown","metadata":{},"source":["**The algorithm performance is not so bad. <br> The F1 score is around 70% for the more populated classes (Negative and Positive emotions), and lower for the Neutral class (F1=0.53).<br>\n","In particular, the overall accuracy is 70%.**"]},{"cell_type":"markdown","metadata":{},"source":["In the next section we will perform the sentiment analysis using BERT."]},{"cell_type":"markdown","metadata":{},"source":["# BERT Sentiment Analysis"]},{"cell_type":"markdown","metadata":{},"source":["We already performed a basic analyis of the tokenized sentences, now we just need to define a custom tokenizer function and call the encode_plus method of the BERT tokenizer."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:50.053144Z","iopub.status.busy":"2021-12-22T21:16:50.052791Z","iopub.status.idle":"2021-12-22T21:16:50.057001Z","shell.execute_reply":"2021-12-22T21:16:50.056176Z","shell.execute_reply.started":"2021-12-22T21:16:50.053103Z"},"trusted":true},"outputs":[],"source":["MAX_LEN=128"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:50.058828Z","iopub.status.busy":"2021-12-22T21:16:50.058416Z","iopub.status.idle":"2021-12-22T21:16:50.066939Z","shell.execute_reply":"2021-12-22T21:16:50.065741Z","shell.execute_reply.started":"2021-12-22T21:16:50.05879Z"},"trusted":true},"outputs":[],"source":["def tokenize(data,max_len=MAX_LEN) :\n","    input_ids = []\n","    attention_masks = []\n","    for i in range(len(data)):\n","        encoded = tokenizer.encode_plus(\n","            data[i],\n","            add_special_tokens=True,\n","            max_length=MAX_LEN,\n","            padding='max_length',\n","            return_attention_mask=True\n","        )\n","        input_ids.append(encoded['input_ids'])\n","        attention_masks.append(encoded['attention_mask'])\n","    return np.array(input_ids),np.array(attention_masks)"]},{"cell_type":"markdown","metadata":{},"source":["Then, we apply the tokenizer function to the train, validation and test sets."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:16:50.068739Z","iopub.status.busy":"2021-12-22T21:16:50.068345Z","iopub.status.idle":"2021-12-22T21:17:05.894027Z","shell.execute_reply":"2021-12-22T21:17:05.89327Z","shell.execute_reply.started":"2021-12-22T21:16:50.068706Z"},"trusted":true},"outputs":[],"source":["train_input_ids, train_attention_masks = tokenize(X_train, MAX_LEN)\n","val_input_ids, val_attention_masks = tokenize(X_valid, MAX_LEN)\n","test_input_ids, test_attention_masks = tokenize(X_test, MAX_LEN)"]},{"cell_type":"markdown","metadata":{},"source":["# BERT modeling"]},{"cell_type":"markdown","metadata":{},"source":["Now we can import the BERT model from the pretrained library from Hugging face."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2021-12-22T21:17:05.89565Z","iopub.status.busy":"2021-12-22T21:17:05.895409Z","iopub.status.idle":"2021-12-22T21:17:08.74259Z","shell.execute_reply":"2021-12-22T21:17:08.741859Z","shell.execute_reply.started":"2021-12-22T21:17:05.895617Z"},"trusted":true},"outputs":[],"source":["bert_model = TFBertModel.from_pretrained('bert-base-uncased')"]},{"cell_type":"markdown","metadata":{},"source":["Then, we create a custom function to host the pre trained BERT model, and attach to it a 3 neurons output layer, necessary to perform the classification of the 3 different classes of the dataset (the 3 emotions)."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:17:08.744637Z","iopub.status.busy":"2021-12-22T21:17:08.744124Z","iopub.status.idle":"2021-12-22T21:17:08.752172Z","shell.execute_reply":"2021-12-22T21:17:08.751417Z","shell.execute_reply.started":"2021-12-22T21:17:08.744596Z"},"trusted":true},"outputs":[],"source":["def create_model(bert_model, max_len=MAX_LEN):\n","    \n","    ##params###\n","    opt = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-7)\n","    loss = tf.keras.losses.CategoricalCrossentropy()\n","    accuracy = tf.keras.metrics.CategoricalAccuracy()\n","\n","\n","    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n","    \n","    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n","    \n","    embeddings = bert_model([input_ids,attention_masks])[1]\n","    \n","    output = tf.keras.layers.Dense(3, activation=\"softmax\")(embeddings)\n","    \n","    model = tf.keras.models.Model(inputs = [input_ids,attention_masks], outputs = output)\n","    \n","    model.compile(opt, loss=loss, metrics=accuracy)\n","    \n","    \n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:17:08.753634Z","iopub.status.busy":"2021-12-22T21:17:08.753301Z","iopub.status.idle":"2021-12-22T21:17:10.212279Z","shell.execute_reply":"2021-12-22T21:17:10.211498Z","shell.execute_reply.started":"2021-12-22T21:17:08.753596Z"},"trusted":true},"outputs":[],"source":["model = create_model(bert_model, MAX_LEN)\n","model.summary()"]},{"cell_type":"markdown","metadata":{},"source":["Finally we can start fine tuning the BERT transformer !"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T21:17:10.213855Z","iopub.status.busy":"2021-12-22T21:17:10.21337Z","iopub.status.idle":"2021-12-22T22:06:54.439353Z","shell.execute_reply":"2021-12-22T22:06:54.438631Z","shell.execute_reply.started":"2021-12-22T21:17:10.213816Z"},"trusted":true},"outputs":[],"source":["history_bert = model.fit([train_input_ids,train_attention_masks], y_train, validation_data=([val_input_ids,val_attention_masks], y_valid), epochs=4, batch_size=32)"]},{"cell_type":"markdown","metadata":{},"source":["# BERT results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T22:06:54.441322Z","iopub.status.busy":"2021-12-22T22:06:54.441058Z","iopub.status.idle":"2021-12-22T22:07:16.077636Z","shell.execute_reply":"2021-12-22T22:07:16.076752Z","shell.execute_reply.started":"2021-12-22T22:06:54.441287Z"},"trusted":true},"outputs":[],"source":["result_bert = model.predict([test_input_ids,test_attention_masks])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T22:07:16.081513Z","iopub.status.busy":"2021-12-22T22:07:16.081165Z","iopub.status.idle":"2021-12-22T22:07:16.087746Z","shell.execute_reply":"2021-12-22T22:07:16.087002Z","shell.execute_reply.started":"2021-12-22T22:07:16.081482Z"},"trusted":true},"outputs":[],"source":["y_pred_bert =  np.zeros_like(result_bert)\n","y_pred_bert[np.arange(len(y_pred_bert)), result_bert.argmax(1)] = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-12-22T22:07:16.089448Z","iopub.status.busy":"2021-12-22T22:07:16.089167Z","iopub.status.idle":"2021-12-22T22:07:16.318816Z","shell.execute_reply":"2021-12-22T22:07:16.318033Z","shell.execute_reply.started":"2021-12-22T22:07:16.089414Z"},"trusted":true},"outputs":[],"source":["conf_matrix(y_test.argmax(1), y_pred_bert.argmax(1),'BERT Sentiment Analysis\\nConfusion Matrix')"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-12-22T22:07:16.320692Z","iopub.status.busy":"2021-12-22T22:07:16.31994Z","iopub.status.idle":"2021-12-22T22:07:16.345718Z","shell.execute_reply":"2021-12-22T22:07:16.345014Z","shell.execute_reply.started":"2021-12-22T22:07:16.320653Z"},"trusted":true},"outputs":[],"source":["print('\\tClassification Report for BERT:\\n\\n',classification_report(y_test,y_pred_bert, target_names=['Negative', 'Neutral', 'Positive']))"]},{"cell_type":"markdown","metadata":{},"source":["# RoBERTa Sentiment Analysis"]},{"cell_type":"markdown","metadata":{},"source":["As seen for BERT, we first import the tokenizer used to train the original roberta transformer by Facebook."]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2021-12-22T22:08:43.116667Z","iopub.status.busy":"2021-12-22T22:08:43.116218Z","iopub.status.idle":"2021-12-22T22:08:50.649025Z","shell.execute_reply":"2021-12-22T22:08:50.648287Z","shell.execute_reply.started":"2021-12-22T22:08:43.116627Z"},"trusted":true},"outputs":[],"source":["tokenizer_roberta = RobertaTokenizerFast.from_pretrained(\"roberta-base\")"]},{"cell_type":"markdown","metadata":{},"source":["First, we check the length of the longest tokenized sentence by roberta tokenizer:"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T22:08:50.651554Z","iopub.status.busy":"2021-12-22T22:08:50.651296Z","iopub.status.idle":"2021-12-22T22:09:00.109456Z","shell.execute_reply":"2021-12-22T22:09:00.108622Z","shell.execute_reply.started":"2021-12-22T22:08:50.651524Z"},"trusted":true},"outputs":[],"source":["token_lens = []\n","\n","for txt in X_train:\n","    tokens = tokenizer_roberta.encode(txt, max_length=512, truncation=True)\n","    token_lens.append(len(tokens))\n","max_length=np.max(token_lens)\n","max_length"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T22:09:00.111182Z","iopub.status.busy":"2021-12-22T22:09:00.110918Z","iopub.status.idle":"2021-12-22T22:09:00.11522Z","shell.execute_reply":"2021-12-22T22:09:00.114446Z","shell.execute_reply.started":"2021-12-22T22:09:00.111146Z"},"trusted":true},"outputs":[],"source":["MAX_LEN=128"]},{"cell_type":"markdown","metadata":{},"source":["Then we can define the tokenization function"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T22:09:00.11775Z","iopub.status.busy":"2021-12-22T22:09:00.117509Z","iopub.status.idle":"2021-12-22T22:09:00.126043Z","shell.execute_reply":"2021-12-22T22:09:00.125367Z","shell.execute_reply.started":"2021-12-22T22:09:00.117704Z"},"trusted":true},"outputs":[],"source":["def tokenize_roberta(data,max_len=MAX_LEN) :\n","    input_ids = []\n","    attention_masks = []\n","    for i in range(len(data)):\n","        encoded = tokenizer_roberta.encode_plus(\n","            data[i],\n","            add_special_tokens=True,\n","            max_length=max_len,\n","            padding='max_length',\n","            return_attention_mask=True\n","        )\n","        input_ids.append(encoded['input_ids'])\n","        attention_masks.append(encoded['attention_mask'])\n","    return np.array(input_ids),np.array(attention_masks)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T22:09:00.128657Z","iopub.status.busy":"2021-12-22T22:09:00.12842Z","iopub.status.idle":"2021-12-22T22:09:16.271659Z","shell.execute_reply":"2021-12-22T22:09:16.270814Z","shell.execute_reply.started":"2021-12-22T22:09:00.128626Z"},"trusted":true},"outputs":[],"source":["train_input_ids, train_attention_masks = tokenize_roberta(X_train, MAX_LEN)\n","val_input_ids, val_attention_masks = tokenize_roberta(X_valid, MAX_LEN)\n","test_input_ids, test_attention_masks = tokenize_roberta(X_test, MAX_LEN)"]},{"cell_type":"markdown","metadata":{},"source":["# RoBERTa modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T22:09:16.27546Z","iopub.status.busy":"2021-12-22T22:09:16.275223Z","iopub.status.idle":"2021-12-22T22:09:16.282419Z","shell.execute_reply":"2021-12-22T22:09:16.28167Z","shell.execute_reply.started":"2021-12-22T22:09:16.275433Z"},"trusted":true},"outputs":[],"source":["def create_model(bert_model, max_len=MAX_LEN):\n","    \n","    opt = tf.keras.optimizers.Adam(learning_rate=1e-5, decay=1e-7)\n","    loss = tf.keras.losses.CategoricalCrossentropy()\n","    accuracy = tf.keras.metrics.CategoricalAccuracy()\n","\n","    input_ids = tf.keras.Input(shape=(max_len,),dtype='int32')\n","    attention_masks = tf.keras.Input(shape=(max_len,),dtype='int32')\n","    output = bert_model([input_ids,attention_masks])\n","    output = output[1]\n","    output = tf.keras.layers.Dense(3, activation=tf.nn.softmax)(output)\n","    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n","    model.compile(opt, loss=loss, metrics=accuracy)\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"execution":{"iopub.execute_input":"2021-12-22T22:09:16.284104Z","iopub.status.busy":"2021-12-22T22:09:16.283698Z","iopub.status.idle":"2021-12-22T22:09:19.200534Z","shell.execute_reply":"2021-12-22T22:09:19.198861Z","shell.execute_reply.started":"2021-12-22T22:09:16.284069Z"},"trusted":true},"outputs":[],"source":["roberta_model = TFRobertaModel.from_pretrained('roberta-base')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T22:09:19.203495Z","iopub.status.busy":"2021-12-22T22:09:19.203061Z","iopub.status.idle":"2021-12-22T22:09:21.496134Z","shell.execute_reply":"2021-12-22T22:09:21.495426Z","shell.execute_reply.started":"2021-12-22T22:09:19.203453Z"},"trusted":true},"outputs":[],"source":["model = create_model(roberta_model, MAX_LEN)\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T22:09:21.497782Z","iopub.status.busy":"2021-12-22T22:09:21.497524Z","iopub.status.idle":"2021-12-22T23:01:57.16071Z","shell.execute_reply":"2021-12-22T23:01:57.159908Z","shell.execute_reply.started":"2021-12-22T22:09:21.497746Z"},"trusted":true},"outputs":[],"source":["history_2 = model.fit([train_input_ids,train_attention_masks], y_train, validation_data=([val_input_ids,val_attention_masks], y_valid), epochs=4, batch_size=30)"]},{"cell_type":"markdown","metadata":{},"source":["# RoBERTa results"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T23:01:57.162787Z","iopub.status.busy":"2021-12-22T23:01:57.162528Z","iopub.status.idle":"2021-12-22T23:02:18.816734Z","shell.execute_reply":"2021-12-22T23:02:18.815967Z","shell.execute_reply.started":"2021-12-22T23:01:57.162754Z"},"trusted":true},"outputs":[],"source":["result_roberta = model.predict([test_input_ids,test_attention_masks])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-12-22T23:02:18.818725Z","iopub.status.busy":"2021-12-22T23:02:18.818471Z","iopub.status.idle":"2021-12-22T23:02:18.824354Z","shell.execute_reply":"2021-12-22T23:02:18.823634Z","shell.execute_reply.started":"2021-12-22T23:02:18.818691Z"},"trusted":true},"outputs":[],"source":["y_pred_roberta =  np.zeros_like(result_roberta)\n","y_pred_roberta[np.arange(len(y_pred_roberta)), result_roberta.argmax(1)] = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-12-22T23:02:18.826841Z","iopub.status.busy":"2021-12-22T23:02:18.826277Z","iopub.status.idle":"2021-12-22T23:02:19.069944Z","shell.execute_reply":"2021-12-22T23:02:19.069179Z","shell.execute_reply.started":"2021-12-22T23:02:18.826804Z"},"trusted":true},"outputs":[],"source":["conf_matrix(y_test.argmax(1),y_pred_roberta.argmax(1),'RoBERTa Sentiment Analysis\\nConfusion Matrix')"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-12-22T23:02:19.071621Z","iopub.status.busy":"2021-12-22T23:02:19.071175Z","iopub.status.idle":"2021-12-22T23:02:19.09548Z","shell.execute_reply":"2021-12-22T23:02:19.094654Z","shell.execute_reply.started":"2021-12-22T23:02:19.071583Z"},"trusted":true},"outputs":[],"source":["print('\\tClassification Report for RoBERTa:\\n\\n',classification_report(y_test,y_pred_roberta, target_names=['Negative', 'Neutral', 'Positive']))"]},{"cell_type":"markdown","metadata":{},"source":["# Results Summary"]},{"cell_type":"markdown","metadata":{},"source":["## BERT Classification Report"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-12-22T23:03:50.712282Z","iopub.status.busy":"2021-12-22T23:03:50.712012Z","iopub.status.idle":"2021-12-22T23:03:50.73796Z","shell.execute_reply":"2021-12-22T23:03:50.737074Z","shell.execute_reply.started":"2021-12-22T23:03:50.712244Z"},"trusted":true},"outputs":[],"source":["print('Classification Report for BERT:\\n',classification_report(y_test,y_pred_bert, target_names=['Negative', 'Neutral', 'Positive']))"]},{"cell_type":"markdown","metadata":{},"source":["## RoBERTa Classification Report"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-12-22T23:03:51.065429Z","iopub.status.busy":"2021-12-22T23:03:51.065166Z","iopub.status.idle":"2021-12-22T23:03:51.088265Z","shell.execute_reply":"2021-12-22T23:03:51.087556Z","shell.execute_reply.started":"2021-12-22T23:03:51.065394Z"},"trusted":true},"outputs":[],"source":["print('Classification Report for RoBERTa:\\n',classification_report(y_test,y_pred_roberta, target_names=['Negative', 'Neutral', 'Positive']))"]},{"cell_type":"markdown","metadata":{},"source":["## Classification Matrix Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"execution":{"iopub.execute_input":"2021-12-22T23:03:51.670767Z","iopub.status.busy":"2021-12-22T23:03:51.670252Z","iopub.status.idle":"2021-12-22T23:03:52.085682Z","shell.execute_reply":"2021-12-22T23:03:52.084962Z","shell.execute_reply.started":"2021-12-22T23:03:51.670727Z"},"trusted":true},"outputs":[],"source":["fig, ax = plt.subplots(1,2,figsize=(9,5.5))\n","\n","labels = ['Negative', 'Neutral', 'Positive']\n","plt.suptitle('Sentiment Analysis Comparison\\n Confusion Matrix', fontsize=20)\n","\n","\n","sns.heatmap(confusion_matrix(y_test.argmax(1),y_pred_bert.argmax(1)), annot=True, cmap=\"Blues\", fmt='g', cbar=False, ax=ax[0], annot_kws={\"size\":25})\n","\n","ax[0].set_title('BERT Classifier', fontsize=20)\n","ax[0].set_yticklabels(labels, fontsize=17);\n","ax[0].set_xticklabels(labels, fontsize=17);\n","ax[0].set_ylabel('Test', fontsize=20)\n","ax[0].set_xlabel('Predicted', fontsize=20)\n","\n","sns.heatmap(confusion_matrix(y_test.argmax(1),y_pred_roberta.argmax(1)), annot=True, cmap=\"Blues\", fmt='g', cbar=False, ax=ax[1], annot_kws={\"size\":25})\n","ax[1].set_title('RoBERTa Classifier', fontsize=20)\n","ax[1].set_yticklabels(labels, fontsize=17);\n","ax[1].set_xticklabels(labels, fontsize=17);\n","ax[1].set_ylabel('Test', fontsize=20)\n","ax[1].set_xlabel('Predicted', fontsize=20)\n","\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{},"source":["We can see that both the algorithms performed well on the classification task, with performance scores around 90%."]},{"cell_type":"markdown","metadata":{},"source":["**Thank you for reading my notebook!! Let me know if you have any question or if you want me to check out your works !! :)**"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":863934,"sourceId":1472453,"sourceType":"datasetVersion"}],"dockerImageVersionId":30152,"isGpuEnabled":false,"isInternetEnabled":false,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3.11.0 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"vscode":{"interpreter":{"hash":"5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"}}},"nbformat":4,"nbformat_minor":4}
